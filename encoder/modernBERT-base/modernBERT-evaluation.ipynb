{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcebdb2",
   "metadata": {},
   "source": [
    "# ModernBERT Argument Mining Evaluation Pipeline\n",
    "\n",
    "This script evaluates a fine-tuned ModernBERT model with LoRA adapters\n",
    "for four argument mining tasks:\n",
    "1. ADU Identification\n",
    "2. ADU Classification  \n",
    "3. Stance Classification\n",
    "\n",
    "## ArgumentMiningEvaluator Class\n",
    "- loads tokenizer and stores model path\n",
    "- defines task_configs for each task, specifying label names and task type\n",
    "- load_model_for_task: loads base model and attached the correct LoRA adapter for each task\n",
    "\n",
    "### Benchmark Data Preperation\n",
    "prepare_benchmark_data: loads, structures the evaluation data (claims, premises, categories)\n",
    "\n",
    "### Prediction Methods\n",
    "- predict_adu_identification: Predicts ADU boundaries in text (token classification)\n",
    "- predict_adu_classification: Classifies a text as claim or premise\n",
    "- predict_stance_classification: Predicts stance (pro/con) between a claim and a premise\n",
    "\n",
    "### Evaluation\n",
    "evaluate_all_tasks: Runs all predictions for all examples, collects predictions and ground truth, computes accuracy, F1 scores, stores error results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb8cc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "464c1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/lenap/Desktop/Armin', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa', '/opt/anaconda3/lib/python3.12/site-packages/setuptools/_vendor', '/var/folders/7b/bpwbm4g168sb_l8mr76jv93w0000gn/T/tmp41r0utec', './ArgumentMining', './ArgumentMining', './ArgumentMining', './ArgumentMining']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./ArgumentMining')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4d8055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.queries import get_quality_data\n",
    "from db.quality_data import data as benchmark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67abfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e3d167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentMiningEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for ModernBERT argument mining models with LoRA adapters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_path: str, adapter_paths: Dict[str, str], device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator\n",
    "        \n",
    "        Args:\n",
    "            base_model_path: Path to the base ModernBERT model\n",
    "            adapter_paths: Dictionary mapping task names to adapter paths\n",
    "                          e.g., {'adu_identification': 'path/to/adu_id_adapter', ...}\n",
    "            device: Device to use for inference\n",
    "        \"\"\"\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.base_model_path = base_model_path\n",
    "        self.adapter_paths = adapter_paths\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        \n",
    "        # Initialize models (will be loaded on demand)\n",
    "        self.models = {}\n",
    "        \n",
    "        # Task configurations\n",
    "        self.task_configs = {\n",
    "            'adu_identification': {\n",
    "                'labels': ['No', 'Yes'],  # 2 classes: No, Yes\n",
    "                'task_type': 'token_classification'\n",
    "        },\n",
    "        'adu_classification': {\n",
    "            'labels': ['claim', 'premise'],  # 2 classes\n",
    "            'task_type': 'sequence_classification'\n",
    "        },\n",
    "        'stance_classification': {\n",
    "            'labels': ['con', 'pro'],  # <-- match fine-tuning order!\n",
    "            'task_type': 'sequence_classification'\n",
    "        },\n",
    "        'relationship_identification': {\n",
    "            'labels': ['contradictory', 'supportive'],  # <-- match fine-tuning order!\n",
    "            'task_type': 'sequence_classification'\n",
    "        }\n",
    "    }\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_model_for_task(self, task_name: str):\n",
    "        \"\"\"Load model with specific LoRA adapter for a task\"\"\"\n",
    "        if task_name in self.models:\n",
    "            return self.models[task_name]\n",
    "            \n",
    "        logger.info(f\"Loading model for task: {task_name}\")\n",
    "        \n",
    "        # Load base model\n",
    "        if self.task_configs[task_name]['task_type'] == 'token_classification':\n",
    "            from transformers import AutoModelForTokenClassification\n",
    "            base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "                self.base_model_path,\n",
    "                num_labels=len(self.task_configs[task_name]['labels'])\n",
    "            )\n",
    "        else:\n",
    "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.base_model_path,\n",
    "                num_labels=len(self.task_configs[task_name]['labels'])\n",
    "            )\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        model = PeftModel.from_pretrained(base_model, self.adapter_paths[task_name])\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        self.models[task_name] = model\n",
    "        return model\n",
    "        \n",
    "    def prepare_benchmark_data(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Prepare benchmark data for evaluation\n",
    "        Returns list of examples with ground truth labels\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing benchmark data...\")\n",
    "        \n",
    "        # Get benchmark data using your existing function or directly from database\n",
    "        try:\n",
    "            # Try to use your existing function\n",
    "            from db.queries import get_quality_data\n",
    "            claims, premises_lists, categories_lists = get_quality_data(benchmark_data)\n",
    "        except ImportError:\n",
    "            # Fallback: directly query database\n",
    "            logger.info(\"Using fallback database query...\")\n",
    "            claims, premises_lists, categories_lists = self._get_benchmark_data_from_db()\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i, (claim, premises_list, categories_list) in enumerate(zip(claims, premises_lists, categories_lists)):\n",
    "            # Create example for each claim-premise pair\n",
    "            for j, (premise, category) in enumerate(zip(premises_list, categories_list)):\n",
    "                example = {\n",
    "                    'claim_id': claim.id,\n",
    "                    'premise_id': premise.id,\n",
    "                    'claim_text': claim.text,\n",
    "                    'premise_text': premise.text,\n",
    "                    'stance': category,  # stance_pro or stance_con\n",
    "                    'claim_type': 'claim',  # Ground truth for ADU classification\n",
    "                    'premise_type': 'premise',  # Ground truth for ADU classification\n",
    "                }\n",
    "                examples.append(example)\n",
    "                \n",
    "        return examples\n",
    "    \n",
    "    def _get_benchmark_data_from_db(self):\n",
    "        \"\"\"\n",
    "        Fallback method to get benchmark data directly from database\n",
    "        \"\"\"\n",
    "        from .db import get_session\n",
    "        from .models import ADU, Relationship\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        with get_session() as session:\n",
    "            claim_ids = list(benchmark_data.keys())\n",
    "            premise_ids = list({pid for pids in benchmark_data.values() for pid in pids})\n",
    "            \n",
    "            # Get claims and premises\n",
    "            claims = session.query(ADU).filter(ADU.id.in_(claim_ids)).all()\n",
    "            premises = session.query(ADU).filter(ADU.id.in_(premise_ids)).all()\n",
    "            \n",
    "            claims_by_id = {c.id: c for c in claims}\n",
    "            premises_by_id = {p.id: p for p in premises}\n",
    "            \n",
    "            # Get relationships\n",
    "            rows = (\n",
    "                session\n",
    "                .query(Relationship.from_adu_id, Relationship.to_adu_id, Relationship.category)\n",
    "                .filter(\n",
    "                    Relationship.to_adu_id.in_(claim_ids),\n",
    "                    Relationship.from_adu_id.in_(premise_ids)\n",
    "                )\n",
    "                .all()\n",
    "            )\n",
    "            \n",
    "            category_lookup = {(from_id, to_id): cat for from_id, to_id, cat in rows}\n",
    "            \n",
    "            output_claims = []\n",
    "            output_premises = []\n",
    "            output_categories = []\n",
    "            \n",
    "            for claim_id, premise_list in benchmark_data.items():\n",
    "                claim = claims_by_id.get(claim_id)\n",
    "                if not claim:\n",
    "                    continue\n",
    "                    \n",
    "                current_premises = []\n",
    "                current_categories = []\n",
    "                \n",
    "                for pid in premise_list:\n",
    "                    premise = premises_by_id.get(pid)\n",
    "                    if not premise:\n",
    "                        continue\n",
    "                        \n",
    "                    current_premises.append(premise)\n",
    "                    category = category_lookup.get((pid, claim_id), None)\n",
    "                    current_categories.append(category)\n",
    "                \n",
    "                output_claims.append(claim)\n",
    "                output_premises.append(current_premises)\n",
    "                output_categories.append(current_categories)\n",
    "            \n",
    "            return output_claims, output_premises, output_categories\n",
    "        \n",
    "    def predict_adu_identification(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict ADU boundaries in text using token classification\n",
    "        Returns list of BIO tags\n",
    "        \"\"\"\n",
    "        model = self.load_model_for_task('adu_identification')\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Convert to labels\n",
    "        labels = self.task_configs['adu_identification']['labels']\n",
    "        pred_labels = [labels[p] for p in predictions[0].cpu().numpy()]\n",
    "        \n",
    "        return pred_labels\n",
    "        \n",
    "    def predict_adu_classification(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify ADU type (claim/premise)\n",
    "        \"\"\"\n",
    "        model = self.load_model_for_task('adu_classification')\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        labels = self.task_configs['adu_classification']['labels']\n",
    "        return labels[prediction.item()]\n",
    "        \n",
    "    def predict_stance_classification(self, claim_text: str, premise_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify stance between claim and premise\n",
    "        \"\"\"\n",
    "        model = self.load_model_for_task('stance_classification')\n",
    "        \n",
    "        # Combine texts (adjust format based on your training)\n",
    "        combined_text = f\"[CLS] {claim_text} [SEP] {premise_text} [SEP]\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            combined_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        labels = self.task_configs['stance_classification']['labels']\n",
    "        return labels[prediction.item()]\n",
    "        \n",
    "    def evaluate_all_tasks(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run evaluation on all tasks and return comprehensive results\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting comprehensive evaluation...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        examples = self.prepare_benchmark_data()\n",
    "        \n",
    "        # Initialize prediction storage\n",
    "        predictions = {\n",
    "            'adu_classification_claim': [],\n",
    "            'adu_classification_premise': [], \n",
    "            'stance_classification': []\n",
    "        }\n",
    "        \n",
    "        ground_truth = {\n",
    "            'adu_classification_claim': [],\n",
    "            'adu_classification_premise': [],\n",
    "            'stance_classification': []\n",
    "        }\n",
    "        \n",
    "        # Run predictions\n",
    "        logger.info(f\"Processing {len(examples)} examples...\")\n",
    "        for example in tqdm(examples):\n",
    "            # ADU Classification\n",
    "            claim_pred = self.predict_adu_classification(example['claim_text'])\n",
    "            premise_pred = self.predict_adu_classification(example['premise_text'])\n",
    "            \n",
    "            predictions['adu_classification_claim'].append(claim_pred)\n",
    "            predictions['adu_classification_premise'].append(premise_pred)\n",
    "            ground_truth['adu_classification_claim'].append(example['claim_type'])\n",
    "            ground_truth['adu_classification_premise'].append(example['premise_type'])\n",
    "            \n",
    "            # Stance Classification\n",
    "            stance_pred = self.predict_stance_classification(\n",
    "                example['claim_text'], \n",
    "                example['premise_text']\n",
    "            )\n",
    "            predictions['stance_classification'].append(stance_pred)\n",
    "\n",
    "            # Map ground truth to model's expected labels\n",
    "            if example['stance'] == 'stance_pro':\n",
    "                stance_gt = 'pro'\n",
    "            elif example['stance'] == 'stance_con':\n",
    "                stance_gt = 'con'\n",
    "            else:\n",
    "                stance_gt = None\n",
    "            ground_truth['stance_classification'].append(stance_gt)\n",
    "            \n",
    "        # Calculate metrics\n",
    "        results = {}\n",
    "        \n",
    "        for task_name in predictions.keys():\n",
    "            y_true = ground_truth[task_name]\n",
    "            y_pred = predictions[task_name]\n",
    "\n",
    "            filtered = [(yt, yp) for yt, yp in zip(y_true, y_pred) if yt is not None and yp is not None]\n",
    "            if not filtered:\n",
    "                continue\n",
    "            y_true_filtered, y_pred_filtered = zip(*filtered)\n",
    "\n",
    "            results[task_name] = {\n",
    "                'accuracy': accuracy_score(y_true_filtered, y_pred_filtered),\n",
    "                'f1_macro': f1_score(y_true_filtered, y_pred_filtered, average='macro'),\n",
    "                'f1_weighted': f1_score(y_true_filtered, y_pred_filtered, average='weighted'),\n",
    "                'classification_report': classification_report(y_true_filtered, y_pred_filtered, output_dict=True),\n",
    "                'confusion_matrix': confusion_matrix(y_true_filtered, y_pred_filtered).tolist(),\n",
    "                'predictions': list(y_pred_filtered),\n",
    "                'ground_truth': list(y_true_filtered)\n",
    "        }\n",
    "            \n",
    "        self.results = results\n",
    "        return results\n",
    "        \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"Print a summary of evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ARGUMENT MINING EVALUATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for task_name, metrics in self.results.items():\n",
    "            print(f\"\\n{task_name.upper().replace('_', ' ')}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "            print(f\"F1 (Weighted): {metrics['f1_weighted']:.4f}\")\n",
    "            \n",
    "            # Print per-class metrics\n",
    "            report = metrics['classification_report']\n",
    "            for label, scores in report.items():\n",
    "                if isinstance(scores, dict) and label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                    print(f\"  {label}: Precision={scores['precision']:.3f}, \"\n",
    "                          f\"Recall={scores['recall']:.3f}, F1={scores['f1-score']:.3f}\")\n",
    "                          \n",
    "    def save_detailed_results(self, output_path: str):\n",
    "        \"\"\"Save detailed results to JSON file\"\"\"\n",
    "        output_file = Path(output_path)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "            \n",
    "        logger.info(f\"Detailed results saved to {output_file}\")\n",
    "        \n",
    "    def create_error_analysis(self) -> pd.DataFrame:\n",
    "        \"\"\"Create error analysis DataFrame\"\"\"\n",
    "        examples = self.prepare_benchmark_data()\n",
    "        \n",
    "        error_data = []\n",
    "        \n",
    "        # for stance classification results\n",
    "        stance_preds = self.results['stance_classification']['predictions']\n",
    "        stance_gt = self.results['stance_classification']['ground_truth']\n",
    "        \n",
    "        for i, (example, pred, gt) in enumerate(zip(examples, stance_preds, stance_gt)):\n",
    "            if pred != gt:\n",
    "                error_data.append({\n",
    "                    'claim_id': example['claim_id'],\n",
    "                    'premise_id': example['premise_id'],\n",
    "                    'claim_text': example['claim_text'][:100] + '...',\n",
    "                    'premise_text': example['premise_text'][:100] + '...',\n",
    "                    'predicted': pred,\n",
    "                    'ground_truth': gt,\n",
    "                    'error_type': f\"{gt} -> {pred}\"\n",
    "                })\n",
    "                \n",
    "        return pd.DataFrame(error_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7d23b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main evaluation function\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_MODEL_PATH = \"answerdotai/ModernBERT-base\"  # Hugging Face model\n",
    "    ADAPTER_PATHS = {\n",
    "        'adu_identification': \"./argument-mining-modernbert-all/argument-mining-modernbert-adu_identification\",\n",
    "        'adu_classification': \"./argument-mining-modernbert-all/argument-mining-modernbert-adu_classification\", \n",
    "        'stance_classification': \"./argument-mining-modernbert-all/argument-mining-modernbert-stance_classification\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ArgumentMiningEvaluator(\n",
    "        base_model_path=BASE_MODEL_PATH,\n",
    "        adapter_paths=ADAPTER_PATHS\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_all_tasks()\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_results_summary()\n",
    "    \n",
    "    # Save detailed results\n",
    "    evaluator.save_detailed_results(\"evaluation_results.json\")\n",
    "    \n",
    "    # Create error analysis\n",
    "    error_df = evaluator.create_error_analysis()\n",
    "    error_df.to_csv(\"error_analysis.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nError analysis saved to error_analysis.csv\")\n",
    "    print(f\"Found {len(error_df)} errors out of {len(evaluator.results['stance_classification']['predictions'])} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "effc5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting comprehensive evaluation...\n",
      "INFO:__main__:Preparing benchmark data...\n",
      "INFO:__main__:Processing 301 examples...\n",
      "  0%|          | 0/301 [00:00<?, ?it/s]INFO:__main__:Loading model for task: adu_classification\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Loading model for task: stance_classification\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 301/301 [06:58<00:00,  1.39s/it]  \n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "INFO:__main__:Detailed results saved to evaluation_results.json\n",
      "INFO:__main__:Preparing benchmark data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ARGUMENT MINING EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "ADU CLASSIFICATION CLAIM\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.9767\n",
      "F1 (Macro): 0.4941\n",
      "F1 (Weighted): 0.9882\n",
      "  claim: Precision=1.000, Recall=0.977, F1=0.988\n",
      "  premise: Precision=0.000, Recall=0.000, F1=0.000\n",
      "\n",
      "ADU CLASSIFICATION PREMISE\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.8804\n",
      "F1 (Macro): 0.4682\n",
      "F1 (Weighted): 0.9364\n",
      "  claim: Precision=0.000, Recall=0.000, F1=0.000\n",
      "  premise: Precision=1.000, Recall=0.880, F1=0.936\n",
      "\n",
      "STANCE CLASSIFICATION\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.5671\n",
      "F1 (Macro): 0.5640\n",
      "F1 (Weighted): 0.5640\n",
      "  con: Precision=0.581, Recall=0.483, F1=0.527\n",
      "  pro: Precision=0.557, Recall=0.651, F1=0.601\n",
      "\n",
      "Error analysis saved to error_analysis.csv\n",
      "Found 129 errors out of 298 predictions\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32edcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
