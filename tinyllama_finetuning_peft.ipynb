{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Horizontal-Labs/training-zoo/blob/main/tinyllama_finetuning_peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trl in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (0.17.0)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from trl) (1.7.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from trl) (3.6.0)\n",
            "Requirement already satisfied: rich in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from trl) (14.0.0)\n",
            "Requirement already satisfied: transformers>=4.46.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from trl) (4.51.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.7.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.31.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (80.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: evaluate in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (2.2.6)\n",
            "Requirement already satisfied: dill in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (0.31.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (0.46.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from bitsandbytes) (2.7.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from bitsandbytes) (2.2.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (80.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: mysql-connector-python in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (9.3.0)\n",
            "Requirement already satisfied: sqlalchemy in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (2.0.41)\n",
            "Requirement already satisfied: mariadb in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (1.1.12)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from sqlalchemy) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from sqlalchemy) (4.13.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (from mariadb) (25.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: protobuf in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (6.31.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\joost\\workspace\\training-zoo\\venv\\lib\\site-packages (0.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install trl\n",
        "%pip install evaluate\n",
        "%pip install -U bitsandbytes\n",
        "%pip install mysql-connector-python sqlalchemy mariadb\n",
        "%pip install protobuf\n",
        "%pip install transformers[sentencepiece]\n",
        "%pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token: \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for the token without displaying it\n",
        "print(\"Enter your Hugging Face token: \")\n",
        "# hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "hf_token = \"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\"\n",
        "os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\n",
        "\n",
        "# Clear the token variable from memory\n",
        "del hf_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enum to choose the model for finetuning\n",
        "class ModelID:\n",
        "    TINYLLAMA = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    DEPSEEK_R1_7B = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "    MISTRAL_7B = \"mistralai/Mistral-7B-v0.3\"                        # For Mistral models you have to agree with sharing contact information on the huggingface website\n",
        "    MISTRAL_7B_INSTRUCT = \"mitralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training parameters\n",
        "class TrainingParams:\n",
        "    def __init__(self, model_id, epochs=3, batch_size=32, learning_rate=0.001, resume_from_checkpoint=None, lower_data_boundary=None, upper_data_boundary=None):\n",
        "        self.model_id = model_id\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.resume_from_checkpoint = resume_from_checkpoint\n",
        "        self.lower_data_boundary = lower_data_boundary\n",
        "        self.upper_data_boundary = upper_data_boundary\n",
        "\n",
        "    def __str__(self):\n",
        "        return (f\"Model ID: {self.model_id}, \"\n",
        "                f\"Epochs: {self.epochs}, \"\n",
        "                f\"Batch Size: {self.batch_size}, \"\n",
        "                f\"Learning Rate: {self.learning_rate}\",\n",
        "                f\"Lower Data Boundary: {self.lower_data_boundary}\",\n",
        "                f\"Upper Data Boundary: {self.upper_data_boundary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_params = TrainingParams(\n",
        "    model_id=ModelID.MISTRAL_7B, # Change this to the desired model ID\n",
        "    epochs=3,\n",
        "    batch_size=2,\n",
        "    learning_rate=0.001,\n",
        "    resume_from_checkpoint=False,  # Set to True if you want to resume from a checkpoint\n",
        "    lower_data_boundary=0.0,\n",
        "    upper_data_boundary=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eimmnld_Nhu3",
        "outputId": "f59299a2-c4b9-4b4a-94a4-299364058109"
      },
      "outputs": [],
      "source": [
        "collab_env = hasattr(__builtins__, 'colab')\n",
        "if collab_env:\n",
        "    os.system('apt update')\n",
        "    os.system('apt install -y libmariadb-dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WBKP7blOb8J",
        "outputId": "b4f71b88-25c2-4cba-9817-7b42c96508c6"
      },
      "outputs": [],
      "source": [
        "if collab_env:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDXPfzTCOea1",
        "outputId": "548cbc1b-e05c-4d34-b332-9eb88367b3cc"
      },
      "outputs": [],
      "source": [
        "# change direcotry\n",
        "if collab_env:\n",
        "    !git clone https://github.com/Horizontal-Labs/Argument-Mining.git\n",
        "    sys.path.append('/content/Argument-Mining')\n",
        "else:\n",
        "    sys.path.append('./Argument-Mining')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az_3O9mqdwzv",
        "outputId": "f54c880f-b7cb-441c-f96d-e6ce581bc485"
      },
      "outputs": [],
      "source": [
        "if collab_env:\n",
        "    cache_dir = '/content/Argument-Mining/.cache'\n",
        "    # Check if the cache folder exists and delete its contents\n",
        "    if os.path.exists(cache_dir):\n",
        "        shutil.rmtree(cache_dir)  # Deletes the entire directory and its contents\n",
        "        print(f\"Cache folder at {cache_dir} has been deleted.\")\n",
        "    else:\n",
        "        print(\"Cache folder does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zdgh4cZeb8M",
        "outputId": "f6eb6671-4d55-420a-e5e7-b4926e188de7"
      },
      "outputs": [],
      "source": [
        "if collab_env:\n",
        "    %cd /content/Argument-Mining/\n",
        "    !git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "n7shxXWjfPQX"
      },
      "outputs": [],
      "source": [
        "if collab_env:\n",
        "    cache_dir = '/content/Argument-Mining/.cache'\n",
        "    os.makedirs(cache_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "P7s4Ag0eO8Wu"
      },
      "outputs": [],
      "source": [
        "from db.queries import get_training_data, get_test_data\n",
        "\n",
        "# Load training data\n",
        "claims_train, premises_train, relationships_train = get_training_data()\n",
        "\n",
        "# Load test data\n",
        "claims_test, premises_test, relationships_test = get_test_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPcFuqLEd7X6",
        "outputId": "e6afe159-57f6-4661-a9b7-b9772fca6be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Claims: 40923\n",
            "Train Premises: 40923\n",
            "Train Relationships: 40923\n",
            "Test Claims: 10395\n",
            "Test Premises: 10395\n",
            "Test Relationships: 10395\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Claims:\", len(claims_train))\n",
        "print(\"Train Premises:\", len(premises_train))\n",
        "print(\"Train Relationships:\", len(relationships_train))\n",
        "\n",
        "print(\"Test Claims:\", len(claims_test))\n",
        "print(\"Test Premises:\", len(claims_test))\n",
        "print(\"Test Relationships:\", len(claims_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n0E2b0sPmBe",
        "outputId": "4e85f40d-c46e-48be-fae8-f4d437e28459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x0000024999E1B1D0>, 'domain_id': 2, 'id': 3, 'text': 'This house believes that the sale of violent video games to minors should be banned', 'type': 'claim'}\n",
            "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x0000024DA4C66570>, 'domain_id': 2, 'id': 5, 'text': 'video game violence is not related to serious aggressive behavior in real life', 'type': 'premise'}\n"
          ]
        }
      ],
      "source": [
        "# Check attributes available in one of the ADU objects\n",
        "print(vars(claims_train[0]))  # Check first claim object\n",
        "print(vars(premises_train[0]))  # Check first premise object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8C2oP90QvRC",
        "outputId": "9e37a614-7eea-4f80-cce6-db2bc9f12f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claims Train has 0 None values.\n",
            "Premises Train has 0 None values.\n",
            "Relationship Train has 0 None values.\n"
          ]
        }
      ],
      "source": [
        "# Check if any ADU is None\n",
        "print(f\"Claims Train has {sum(x is None for x in claims_train)} None values.\")\n",
        "print(f\"Premises Train has {sum(x is None for x in premises_train)} None values.\")\n",
        "print(f\"Relationship Train has {sum(x is None for x in relationships_train)} None values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rPUHo5kPU-M",
        "outputId": "ad2f9636-696d-48fb-c16e-c04684883456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               claim  \\\n",
            "0  This house believes that the sale of violent v...   \n",
            "1  This house supports the one-child policy of th...   \n",
            "2  This house would permit the use of performance...   \n",
            "3  This house would make physical education compu...   \n",
            "4  This house believes in the use of affirmative ...   \n",
            "\n",
            "                                             premise      stance  \n",
            "0  video game violence is not related to serious ...  stance_con  \n",
            "1         The policy had proved remarkably effective  stance_pro  \n",
            "2  The use of drugs to enhance performance is con...  stance_con  \n",
            "3  Frequent and regular physical exercise boosts ...  stance_pro  \n",
            "4  In some countries which have laws on racial eq...  stance_con  \n"
          ]
        }
      ],
      "source": [
        "# Create pairs of claims and premises\n",
        "debate_pairs = []\n",
        "\n",
        "for i in range(len(claims_train)):\n",
        "    debate_pairs.append({\n",
        "        \"claim\": claims_train[i].text,\n",
        "        \"premise\": premises_train[i].text,\n",
        "        \"stance\": relationships_train[i],\n",
        "    })\n",
        "\n",
        "# Create final DataFrame\n",
        "train_data = pd.DataFrame(debate_pairs)\n",
        "\n",
        "# Split Training Data\n",
        "if training_params.lower_data_boundary or training_params.upper_data_boundary:\n",
        "    train_data = train_data[int(training_params.lower_data_boundary*len(train_data)):int(training_params.upper_data_boundary*len(train_data))]\n",
        "\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrI2x4RzZV_f",
        "outputId": "400ef60e-c73f-4e05-ddf8-ef9d6986681e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               claim  \\\n",
            "0  This house believes that the sale of violent v...   \n",
            "1  This house supports the one-child policy of th...   \n",
            "2  This house would permit the use of performance...   \n",
            "3  This house would make physical education compu...   \n",
            "4  This house believes in the use of affirmative ...   \n",
            "\n",
            "                                             premise stance  \n",
            "0  video game violence is not related to serious ...    con  \n",
            "1         The policy had proved remarkably effective    pro  \n",
            "2  The use of drugs to enhance performance is con...    con  \n",
            "3  Frequent and regular physical exercise boosts ...    pro  \n",
            "4  In some countries which have laws on racial eq...    con  \n"
          ]
        }
      ],
      "source": [
        "# Remove 'stance_' prefix for simplicity\n",
        "train_data['stance'] = train_data['stance'].str.replace('stance_', '')\n",
        "\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQhMttPkXHFe"
      },
      "source": [
        "# **Finetuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XgDK0aSWQmpT"
      },
      "outputs": [],
      "source": [
        "import torch # for model training and tensor operations\n",
        "from datasets import Dataset # interface for working with datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,      # loads a pre-trained CLM\n",
        "    AutoTokenizer,             # loads corresponding tokenizer for a model\n",
        "    BitsAndBytesConfig,        # configuration for quantization techniques\n",
        "    TrainingArguments,          # holds arguments for training\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning)\n",
        "from peft import (\n",
        "    get_peft_model,               # wraps the base model with PEFT capabilities\n",
        "    LoraConfig,                   # configuration for LoRA (Low-Rank Adaptation)\n",
        "    TaskType,                     # specifies the type of task (e.g. CLM)\n",
        "    prepare_model_for_kbit_training,  # prepares a quantized model\n",
        "    PeftModel                     # class for loading and managing PEFT models\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer          # Trainer for supervised fine-tuning of language models.\n",
        "import evaluate # HF library to compute evaluation metrics for ML models\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report # common metrics for classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_shards(train_data, eval_data, num_shards=32):\n",
        "    \"\"\"\n",
        "    Splits train_data and eval_data into shards and saves them in a dictionary.\n",
        "\n",
        "    Args:\n",
        "        train_data (pd.DataFrame): The training dataset.\n",
        "        eval_data (pd.DataFrame): The evaluation dataset.\n",
        "        num_shards (int): The number of shards to create.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing shards of train_data and eval_data.\n",
        "    \"\"\"\n",
        "    shards = {\"train_shards\": {}, \"eval_shards\": {}}\n",
        "\n",
        "    # Split train_data into shards\n",
        "    train_shards = np.array_split(train_data, num_shards)\n",
        "    for i, shard in enumerate(train_shards):\n",
        "        shards[\"train_shards\"][f\"shard_{i+1}\"] = shard\n",
        "\n",
        "    # Split eval_data into shards\n",
        "    eval_shards = np.array_split(eval_data, num_shards)\n",
        "    for i, shard in enumerate(eval_shards):\n",
        "        shards[\"eval_shards\"][f\"shard_{i+1}\"] = shard\n",
        "\n",
        "    return shards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZQzEYL4TY6Ti"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 32 train shards and 32 eval shards.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, eval_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "shards = create_shards(train_data, eval_data)\n",
        "print(f\"Created {len(shards['train_shards'])} train shards and {len(shards['eval_shards'])} eval shards.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcKqUi4lbSt3"
      },
      "source": [
        "# Data Formatting for Multitask Learning with Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Y8tYcdTbabWr"
      },
      "outputs": [],
      "source": [
        "def format_for_argument_mining(df):\n",
        "    formatted_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Format with instructions for each task\n",
        "\n",
        "        # Task 1: ADU Identification - Extract ADUs from text\n",
        "        # Samples for ADU identification using both claims and premises\n",
        "        claim_adu_sample = {\n",
        "            \"instruction\": \"Identify whether the following text contains an Argumentative Discourse Unit (ADU). An ADU is a span of text that serves as a claim or premise in an argument.\",\n",
        "            \"input\": f\"Text: {row['claim']}\",\n",
        "            \"output\": \"Yes, this text contains an ADU. It functions as a claim in an argument.\"\n",
        "        }\n",
        "\n",
        "        premise_adu_sample = {\n",
        "            \"instruction\": \"Identify whether the following text contains an Argumentative Discourse Unit (ADU). An ADU is a span of text that serves as a claim or premise in an argument.\",\n",
        "            \"input\": f\"Text: {row['premise']}\",\n",
        "            \"output\": \"Yes, this text contains an ADU. It functions as a premise in an argument.\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # Task 2: ADU Classification (determine if claim or premise)\n",
        "        adu_class_sample = {\n",
        "            \"instruction\": \"Classify the following Argumentative Discourse Unit (ADU) as either a claim or premise. A claim is the main point being argued, while a premise provides support or evidence.\",\n",
        "            \"input\": f\"ADU: {row['claim']}\",\n",
        "            \"output\": \"This ADU is a claim.\"\n",
        "        }\n",
        "\n",
        "        adu_class_sample2 = {\n",
        "            \"instruction\": \"Classify the following Argumentative Discourse Unit (ADU) as either a claim or premise. A claim is the main point being argued, while a premise provides support or evidence.\",\n",
        "            \"input\": f\"ADU: {row['premise']}\",\n",
        "            \"output\": \"This ADU is a premise.\"\n",
        "        }\n",
        "\n",
        "        # Task 3: Stance Classification\n",
        "        stance_sample = {\n",
        "            \"instruction\": \"Determine if the premise supports or counters the given claim.\",\n",
        "            \"input\": f\"Claim: {row['claim']}\\nPremise: {row['premise']}\",\n",
        "            \"output\": f\"{row['stance']}\"\n",
        "        }\n",
        "\n",
        "        # Task 4: Relationship identification between ADUs\n",
        "        relationship_sample = {\n",
        "            \"instruction\": \"Identify the relationship between the following claim and premise. Explain how they are connected in the argument structure.\",\n",
        "            \"input\": f\"Claim: {row['claim']}\\nPremise: {row['premise']}\",\n",
        "            \"output\": f\"The premise {'supports' if row['stance'] == 'pro' else 'counters'} the claim. The relationship is {'supportive' if row['stance'] == 'pro' else 'contradictory'}, where the premise provides {'evidence for' if row['stance'] == 'pro' else 'evidence against'} the main argument.\"\n",
        "        }\n",
        "\n",
        "        # Add all tasks to our dataset\n",
        "        formatted_data.extend([\n",
        "            claim_adu_sample,\n",
        "            premise_adu_sample,\n",
        "            adu_class_sample,\n",
        "            adu_class_sample2,\n",
        "            stance_sample,\n",
        "            relationship_sample\n",
        "        ])\n",
        "\n",
        "    return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "rVr-vMm40DbK"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example, tokenizer, max_length=1024):\n",
        "    # Combine instruction and input text as prompt\n",
        "    prompt = example['instruction'] + \"\\n\" + example['input']\n",
        "    target = example['output']\n",
        "\n",
        "    # Tokenize inputs (prompt)\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (outputs/labels)\n",
        "    targets = tokenizer(\n",
        "        target,\n",
        "        max_length=256,  # output length limit\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Prepare labels, replacing pad token id with -100 (ignore index)\n",
        "    labels = targets[\"input_ids\"].copy()\n",
        "    labels = [l if l != tokenizer.pad_token_id else -100 for l in labels]\n",
        "\n",
        "    # Return dictionary for dataset\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h34Q4g68bbGz"
      },
      "source": [
        "# Language Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mbTBoTPzbckP"
      },
      "outputs": [],
      "source": [
        "# Returns both the model and tokenizer, ready for fine-tuning or inference\n",
        "def setup_model(model_id=training_params.model_id):\n",
        "    try:\n",
        "        # QLoRA configuration - use 4-bit quantization for memory efficiency\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        # Load pre-trained model with quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config, # see above\n",
        "            device_map=\"auto\", # maps model layers automatically to available CPU/GPUs\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load correspondent tokenizer for the model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\" # padding directions for filler tokens\n",
        "\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQB_aM_actYi"
      },
      "source": [
        "# PEFT LoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "eF4-YEkGcnb9"
      },
      "outputs": [],
      "source": [
        "def configure_peft(model):\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=32, # scales output of LoRA adapter before its added back to base model weights\n",
        "        lora_dropout=0.1,\n",
        "        # Target only key attention modules\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        bias=\"none\",       # Don't train biases for more stability\n",
        "    )\n",
        "\n",
        "    # Prepare model for training without gradient checkpointing (trade off memory vs slowing down training)\n",
        "    # model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print trainable parameters info\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "t-lfal1ndKAG"
      },
      "outputs": [],
      "source": [
        "def setup_training(model, train_dataset, eval_dataset, output_dir=training_params.model_id):\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"models/\" + output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-4,            # Low-ish learning rate for stability\n",
        "        weight_decay=0.05,             # weight decay to combat overfitting\n",
        "        logging_steps=20,\n",
        "        save_strategy=\"epoch\",\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        fp16=True,\n",
        "        eval_strategy=\"epoch\",   # Add evaluation during training\n",
        "        save_total_limit=2,            # Save disk space\n",
        "        load_best_model_at_end=True,   # Automatically use best model\n",
        "        remove_unused_columns=False    # Keep all columns for potential use\n",
        "    )\n",
        "\n",
        "    # Set up the trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset= eval_dataset\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BDb1ur6qdiK9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_data, task=\"stance\"):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for item in test_data:\n",
        "        if task == \"stance\":\n",
        "            prompt = f\"Instruction: Determine if the premise supports or counters the given claim.\\nInput: Claim: {item['claim']}\\nPremise: {item['premise']}\\nOutput:\"\n",
        "        elif task == \"adu_identification\":\n",
        "            prompt = f\"Instruction: Identify whether the following text contains an Argumentative Discourse Unit (ADU).\\nInput: Text: {item['text']}\\nOutput:\"\n",
        "        elif task == \"adu_classification\":\n",
        "            prompt = f\"Instruction: Classify the following Argumentative Discourse Unit (ADU) as either a claim or premise.\\nInput: ADU: {item['text']}\\nOutput:\"\n",
        "        elif task == \"relationship\":\n",
        "            prompt = f\"Instruction: Identify the relationship between the following claim and premise.\\nInput: Claim: {item['claim']}\\nPremise: {item['premise']}\\nOutput:\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.1,\n",
        "                do_sample=False,\n",
        "                num_beams=1\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = generated_text.split(\"Output:\")[-1].strip().lower()\n",
        "\n",
        "        # Extract relevant information based on task\n",
        "        if task == \"stance\":\n",
        "            if \"pro\" in prediction:\n",
        "                predictions.append(\"pro\")\n",
        "            elif \"con\" in prediction:\n",
        "                predictions.append(\"con\")\n",
        "            else:\n",
        "                predictions.append(\"unknown\")\n",
        "\n",
        "            references.append(item['stance'])\n",
        "\n",
        "        elif task == \"adu_identification\":\n",
        "            if \"yes\" in prediction:\n",
        "                predictions.append(\"contains_adu\")\n",
        "            elif \"no\" in prediction:\n",
        "                predictions.append(\"no_adu\")\n",
        "            else:\n",
        "                predictions.append(\"unknown\")\n",
        "\n",
        "            references.append(item['contains_adu'])\n",
        "\n",
        "        elif task == \"adu_classification\":\n",
        "            if \"claim\" in prediction:\n",
        "                predictions.append(\"claim\")\n",
        "            elif \"premise\" in prediction:\n",
        "                predictions.append(\"premise\")\n",
        "            else:\n",
        "                predictions.append(\"unknown\")\n",
        "\n",
        "            references.append(item['adu_type'])\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(references, predictions)\n",
        "\n",
        "    # For binary tasks like ADU identification\n",
        "    if task == \"adu_identification\":\n",
        "        f1 = f1_score(references, predictions, average='binary', pos_label=\"contains_adu\")\n",
        "    # For multi-class tasks\n",
        "    else:\n",
        "        f1 = f1_score(references, predictions, average='weighted')\n",
        "\n",
        "    print(f\"Task: {task}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(classification_report(references, predictions))\n",
        "\n",
        "    return accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "H3UuAhuYoL0H"
      },
      "outputs": [],
      "source": [
        "# Training Pipeline\n",
        "\n",
        "def train_argument_mining_model(\n",
        "        train_data,\n",
        "        model_id=training_params.model_id,\n",
        "        resume_from_checkpoint=None,\n",
        "        ):\n",
        "    \"\"\"\n",
        "    Train an argument mining model using the provided training data.\n",
        "    Args:\n",
        "        resume_from_checkpoint (`str` or `bool`, *optional*):\n",
        "        If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n",
        "        `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n",
        "        of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
        "    \"\"\"\n",
        "    # Setup model and tokenizer\n",
        "    model, tokenizer = setup_model(model_id)\n",
        "\n",
        "    # Format data for training\n",
        "    formatted_train_data = format_for_argument_mining(train_data)\n",
        "    formatted_eval_data = format_for_argument_mining(eval_data)\n",
        "\n",
        "    # Create HF Dataset from list of dicts\n",
        "    hf_train_dataset = Dataset.from_list(formatted_train_data)\n",
        "    hf_eval_dataset = Dataset.from_list(formatted_eval_data)\n",
        "\n",
        "    # Apply tokenizer function to the train dataset (batched for speed)\n",
        "    train_dataset = hf_train_dataset.map(\n",
        "    lambda x: tokenize_function(x, tokenizer),\n",
        "    batched=False\n",
        "    )\n",
        "\n",
        "    # Apply tokenizer function to the eval dataset (batched for speed)\n",
        "    eval_dataset = hf_eval_dataset.map(\n",
        "    lambda x: tokenize_function(x, tokenizer),\n",
        "    batched=False\n",
        "    )\n",
        "\n",
        "    # Configure PEFT/LoRA\n",
        "    model = configure_peft(model)\n",
        "\n",
        "    # Setup training\n",
        "    trainer = setup_training(model, train_dataset, eval_dataset)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    peft_model_id = f\"argument-mining-{model_id.split('/')[-1]}\"\n",
        "    trainer.model.save_pretrained(peft_model_id)\n",
        "    tokenizer.save_pretrained(peft_model_id)\n",
        "\n",
        "    print(f\"Model saved to {peft_model_id}\")\n",
        "    return model, tokenizer, peft_model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "gPykhuy4oomw"
      },
      "outputs": [],
      "source": [
        "def run_inference(model, tokenizer, text, task=\"stance\", claim=None, premise=None):\n",
        "    \"\"\"\n",
        "    Run inference on different argument mining tasks\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model\n",
        "        tokenizer: The tokenizer\n",
        "        text: Text to analyze (for ADU tasks)\n",
        "        task: Which task to perform - \"adu_identification\", \"adu_classification\", \"stance\", or \"relationship\"\n",
        "        claim: The claim text (for stance and relationship tasks)\n",
        "        premise: The premise text (for stance and relationship tasks)\n",
        "\n",
        "    Returns:\n",
        "        Prediction result as a string\n",
        "    \"\"\"\n",
        "    if task == \"stance\":\n",
        "        if not claim or not premise:\n",
        "            return \"Error: Claim and premise required for stance classification\"\n",
        "        prompt = f\"Instruction: Determine if the premise supports or counters the given claim.\\nInput: Claim: {claim}\\nPremise: {premise}\\nOutput:\"\n",
        "\n",
        "    elif task == \"adu_identification\":\n",
        "        prompt = f\"Instruction: Identify whether the following text contains an Argumentative Discourse Unit (ADU). An ADU is a span of text that serves as a claim or premise in an argument.\\nInput: Text: {text}\\nOutput:\"\n",
        "\n",
        "    elif task == \"adu_classification\":\n",
        "        prompt = f\"Instruction: Classify the following Argumentative Discourse Unit (ADU) as either a claim or premise. A claim is the main point being argued, while a premise provides support or evidence.\\nInput: ADU: {text}\\nOutput:\"\n",
        "\n",
        "    elif task == \"relationship\":\n",
        "        if not claim or not premise:\n",
        "            return \"Error: Claim and premise required for relationship identification\"\n",
        "        prompt = f\"Instruction: Identify the relationship between the following claim and premise. Explain how they are connected in the argument structure.\\nInput: Claim: {claim}\\nPremise: {premise}\\nOutput:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=False  # Deterministic generation for evaluation\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction = generated_text.split(\"Output:\")[-1].strip()\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMPC0_TFoeXU"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471,
          "referenced_widgets": [
            "3265762bb32f4ff28831536de082625e",
            "4bd75baf5a5c4b5798714477cb5c5fe7",
            "996430f5af4b4e83beaab1febb62d577",
            "71d0ab773d844761b9e2ff8ce8cdadbc",
            "fb8172f66ac744e7b4983af9aa972b1e",
            "a9c2be161bee4d04961eca989231858b",
            "18e6af3bff404e5c8bc6105007dce9f0",
            "d0a683c4703147f1939091cd28eceeb9",
            "e2ac8aeb337b405da2678ec9e3cd366b",
            "d304b17c9f314e4aad6bc8d78ca68dc0",
            "b3c9eee97e964a7fb62a67acb12a6472",
            "0f5b95ed714041dd8bd384b979c0ddd8",
            "a05c3f15474c4168ba56650f4a421fd9",
            "d76b88275fc64d48a103b496781d87c1",
            "60982904c13f4f90bd537c96e81c1f8c",
            "d03250a1c179443ca41d229d3194a2cb",
            "07170c736961404897d27d89cd701837",
            "19ba042589574e58b719aa485531a6b8",
            "cfc6c94645644153a473f123efea130e",
            "9f72d5da7dc4493486793b635c7b51ea",
            "d5f40886f27e4c8f8bc4f3d89477796a",
            "00ebff35d1df4cdc821482d43f64017a",
            "f5d14bea2801417cabc7e2040eef42d0",
            "199b1e23fad84a84a26f6019807732b6",
            "c2946436eba14bdf94670abc952d937b",
            "974768749c074cadbe855fdcc40059a0",
            "94b7cc49dabd437e94d94723265a7060",
            "bfec4c79d859467582b126891373475b",
            "4ed510fe8efe4f268b95663b27fe5845",
            "fb786e336fc244bfa4ec3c9e9735c8f5",
            "149d4c930ca94fd5acb8938188dd2576",
            "0d0b3f54a93a48d49b4530c58222ab2e",
            "d86be779c3b44f59a966969e4e80f968",
            "b997d61181dc4c0b81668bbdc37ffe70",
            "c21c123698dc48bc82dfe7ca2d06f22e",
            "770b22f990b642ff9cd94df3d06ab227",
            "767cabaf0a794ab184e1676828503088",
            "ad9fa90beb0549159258bf94f2deb222",
            "b9a276894e894cc1a171e5c8ea9331da",
            "135b17a3a3f04cd4805db6297a80918b",
            "eb5065ede8564239a5b452b755da14b4",
            "745f595c2ff0472fb24f5b0d9056ba41",
            "8f2ed03677204398b97587a4cdfc6d01",
            "cd3beb3df02a48d998c5f6579544584f"
          ]
        },
        "id": "_zyf2b8woXVx",
        "outputId": "7b15bb59-85f1-4d85-dca0-f764d34287df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce RTX 3090\n",
            "Starting training pipeline...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading model: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.get_device_name(\u001b[32m0\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model, tokenizer, model_id = \u001b[43mtrain_argument_mining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Example inference for all tasks\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Example Inferences ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_argument_mining_model\u001b[39m\u001b[34m(train_data, model_id, resume_from_checkpoint)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mTrain an argument mining model using the provided training data.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33;03m    of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Setup model and tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m model, tokenizer = \u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Format data for training\u001b[39;00m\n\u001b[32m     20\u001b[39m formatted_train_data = format_for_argument_mining(train_data)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36msetup_model\u001b[39m\u001b[34m(model_id)\u001b[39m\n\u001b[32m     13\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     14\u001b[39m     model_id,\n\u001b[32m     15\u001b[39m     quantization_config=bnb_config, \u001b[38;5;66;03m# see above\u001b[39;00m\n\u001b[32m     16\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# maps model layers automatically to available CPU/GPUs\u001b[39;00m\n\u001b[32m     17\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load correspondent tokenizer for the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     23\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1009\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1006\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1007\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1008\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2062\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2059\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2302\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2304\u001b[39m     logger.info(\n\u001b[32m   2305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2307\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama_fast.py:157\u001b[39m, in \u001b[36mLlamaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfrom_slow\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m._add_bos_token = add_bos_token\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m._add_eos_token = add_eos_token\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joost\\workspace\\training-zoo\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:108\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.add_prefix_space = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madd_prefix_space\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    109\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms based on sentencepiece, make sure you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhave sentencepiece installed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m     )\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    114\u001b[39m     fast_tokenizer = copy.deepcopy(tokenizer_object)\n",
            "\u001b[31mValueError\u001b[39m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load your data\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "    print(\"Starting training pipeline...\")\n",
        "    model, tokenizer, model_id = train_argument_mining_model(train_data, resume_from_checkpoint=training_params.resume_from_checkpoint)\n",
        "\n",
        "    # Example inference for all tasks\n",
        "    print(\"\\n--- Example Inferences ---\")\n",
        "\n",
        "    # 1. ADU Identification example\n",
        "    example_text = \"The government should invest more in renewable energy.\"\n",
        "    adu_prediction = run_inference(model, tokenizer, example_text, task=\"adu_identification\")\n",
        "    print(f\"ADU Identification: {adu_prediction}\")\n",
        "\n",
        "    # 2. ADU Classification example\n",
        "    example_adu = \"Studies show that renewable energy creates more jobs than fossil fuels.\"\n",
        "    class_prediction = run_inference(model, tokenizer, example_adu, task=\"adu_classification\")\n",
        "    print(f\"ADU Classification: {class_prediction}\")\n",
        "\n",
        "    # 3. Stance Classification example\n",
        "    example_claim = \"This house believes that social media is harmful to society.\"\n",
        "    example_premise = \"Social media has been linked to increased rates of depression in teenagers.\"\n",
        "    stance_prediction = run_inference(model, tokenizer, None, task=\"stance\",\n",
        "                                     claim=example_claim, premise=example_premise)\n",
        "    print(f\"Stance Classification: {stance_prediction}\")\n",
        "\n",
        "    # 4. Relationship Identification example\n",
        "    relationship_prediction = run_inference(model, tokenizer, None, task=\"relationship\",\n",
        "                                           claim=example_claim, premise=example_premise)\n",
        "    print(f\"Relationship Identification: {relationship_prediction}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPeJElPvdiF+vcM5KrMRapt",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00ebff35d1df4cdc821482d43f64017a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07170c736961404897d27d89cd701837": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0b3f54a93a48d49b4530c58222ab2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5b95ed714041dd8bd384b979c0ddd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a05c3f15474c4168ba56650f4a421fd9",
              "IPY_MODEL_d76b88275fc64d48a103b496781d87c1",
              "IPY_MODEL_60982904c13f4f90bd537c96e81c1f8c"
            ],
            "layout": "IPY_MODEL_d03250a1c179443ca41d229d3194a2cb"
          }
        },
        "135b17a3a3f04cd4805db6297a80918b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "149d4c930ca94fd5acb8938188dd2576": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18e6af3bff404e5c8bc6105007dce9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "199b1e23fad84a84a26f6019807732b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfec4c79d859467582b126891373475b",
            "placeholder": "",
            "style": "IPY_MODEL_4ed510fe8efe4f268b95663b27fe5845",
            "value": "Truncatingtraindataset:100%"
          }
        },
        "19ba042589574e58b719aa485531a6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3265762bb32f4ff28831536de082625e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bd75baf5a5c4b5798714477cb5c5fe7",
              "IPY_MODEL_996430f5af4b4e83beaab1febb62d577",
              "IPY_MODEL_71d0ab773d844761b9e2ff8ce8cdadbc"
            ],
            "layout": "IPY_MODEL_fb8172f66ac744e7b4983af9aa972b1e"
          }
        },
        "4bd75baf5a5c4b5798714477cb5c5fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c2be161bee4d04961eca989231858b",
            "placeholder": "",
            "style": "IPY_MODEL_18e6af3bff404e5c8bc6105007dce9f0",
            "value": "Map:100%"
          }
        },
        "4ed510fe8efe4f268b95663b27fe5845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60982904c13f4f90bd537c96e81c1f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f40886f27e4c8f8bc4f3d89477796a",
            "placeholder": "",
            "style": "IPY_MODEL_00ebff35d1df4cdc821482d43f64017a",
            "value": "49110/49110[02:42&lt;00:00,284.72examples/s]"
          }
        },
        "71d0ab773d844761b9e2ff8ce8cdadbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d304b17c9f314e4aad6bc8d78ca68dc0",
            "placeholder": "",
            "style": "IPY_MODEL_b3c9eee97e964a7fb62a67acb12a6472",
            "value": "196428/196428[10:56&lt;00:00,380.11examples/s]"
          }
        },
        "745f595c2ff0472fb24f5b0d9056ba41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "767cabaf0a794ab184e1676828503088": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f2ed03677204398b97587a4cdfc6d01",
            "placeholder": "",
            "style": "IPY_MODEL_cd3beb3df02a48d998c5f6579544584f",
            "value": "49110/49110[00:00&lt;00:00,64959.49examples/s]"
          }
        },
        "770b22f990b642ff9cd94df3d06ab227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb5065ede8564239a5b452b755da14b4",
            "max": 49110,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_745f595c2ff0472fb24f5b0d9056ba41",
            "value": 49110
          }
        },
        "8f2ed03677204398b97587a4cdfc6d01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b7cc49dabd437e94d94723265a7060": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974768749c074cadbe855fdcc40059a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0b3f54a93a48d49b4530c58222ab2e",
            "placeholder": "",
            "style": "IPY_MODEL_d86be779c3b44f59a966969e4e80f968",
            "value": "196428/196428[00:04&lt;00:00,73513.82examples/s]"
          }
        },
        "996430f5af4b4e83beaab1febb62d577": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a683c4703147f1939091cd28eceeb9",
            "max": 196428,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2ac8aeb337b405da2678ec9e3cd366b",
            "value": 196428
          }
        },
        "9f72d5da7dc4493486793b635c7b51ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05c3f15474c4168ba56650f4a421fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07170c736961404897d27d89cd701837",
            "placeholder": "",
            "style": "IPY_MODEL_19ba042589574e58b719aa485531a6b8",
            "value": "Map:100%"
          }
        },
        "a9c2be161bee4d04961eca989231858b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9fa90beb0549159258bf94f2deb222": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c9eee97e964a7fb62a67acb12a6472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b997d61181dc4c0b81668bbdc37ffe70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c21c123698dc48bc82dfe7ca2d06f22e",
              "IPY_MODEL_770b22f990b642ff9cd94df3d06ab227",
              "IPY_MODEL_767cabaf0a794ab184e1676828503088"
            ],
            "layout": "IPY_MODEL_ad9fa90beb0549159258bf94f2deb222"
          }
        },
        "b9a276894e894cc1a171e5c8ea9331da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfec4c79d859467582b126891373475b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21c123698dc48bc82dfe7ca2d06f22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a276894e894cc1a171e5c8ea9331da",
            "placeholder": "",
            "style": "IPY_MODEL_135b17a3a3f04cd4805db6297a80918b",
            "value": "Truncatingevaldataset:100%"
          }
        },
        "c2946436eba14bdf94670abc952d937b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb786e336fc244bfa4ec3c9e9735c8f5",
            "max": 196428,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_149d4c930ca94fd5acb8938188dd2576",
            "value": 196428
          }
        },
        "cd3beb3df02a48d998c5f6579544584f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfc6c94645644153a473f123efea130e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03250a1c179443ca41d229d3194a2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a683c4703147f1939091cd28eceeb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d304b17c9f314e4aad6bc8d78ca68dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f40886f27e4c8f8bc4f3d89477796a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76b88275fc64d48a103b496781d87c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfc6c94645644153a473f123efea130e",
            "max": 49110,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f72d5da7dc4493486793b635c7b51ea",
            "value": 49110
          }
        },
        "d86be779c3b44f59a966969e4e80f968": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2ac8aeb337b405da2678ec9e3cd366b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb5065ede8564239a5b452b755da14b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d14bea2801417cabc7e2040eef42d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_199b1e23fad84a84a26f6019807732b6",
              "IPY_MODEL_c2946436eba14bdf94670abc952d937b",
              "IPY_MODEL_974768749c074cadbe855fdcc40059a0"
            ],
            "layout": "IPY_MODEL_94b7cc49dabd437e94d94723265a7060"
          }
        },
        "fb786e336fc244bfa4ec3c9e9735c8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb8172f66ac744e7b4983af9aa972b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
